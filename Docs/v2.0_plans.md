General:
--------
* Test load collectors
* Test VM collectors
* Performance info collection code is the same as in wally.
  Need to unify this.

  
Performance info collection:
----------------------------
* Monitor per vm activity, fill with zeros missing data
* Show load heatmap/histo for vms
* Show list of most io heavy vm
* Show rbd info for all rbd pools - num images, avg size, etc
* Show volume size histo
* should not fail on unknown operation - just collect them separatelly
* More performance data collected. Flamegraph collected.
  Separated flamegraph for loaded/slow OSD's.
* blktrace
* Latency heatmap
* Don't use ftrace if not explicitly allowed, as it may crush a kernel

Estimate caches size:
  * KDE on histo
  * find a numeric extremums
  * calculate histo cum size between minimumss

Collect:
--------
* Store headers for all metrics with units in it
* /proc/net/netstat
* OSD logging levels
* Aggregate osd by settings
* Better parallelism
* Ceph perf dump
* Logs from down OSD
* Analyze filestore logs to get a typical load profile
* max-sectors-kb linux setting - ????
* dump /sys/block/<dev>/queue
* github.com/cernceph/ceph-scripts
* openstack-in-production.blogspot.com,
  mascetti, disk storage at cern, CHEP 2015
  find flickr paper about ceph
  cds.cern.ch/record/2015206


Report:
-------
* vm localization in 2d (avg blocksize / iops, or bw/avg. block size)
* timerange for heatmap
* Draw network connection diagram
* Network errors rate
* Avg packet size for adapters
* In all reports merge equal lines
* CRUSH visualization:
    - Visualize osd roots separatelly
    - Use pre-calculated graps
    - don't show OSD on graph, only start from hosts
* Fix OSD stage time plot


Need to think:
--------------
* Auto-install perf, ceph-devel, smart-tools
* Trace operations using ftrace
